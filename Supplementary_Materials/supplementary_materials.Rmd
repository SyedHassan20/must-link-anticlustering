---
title: "Supplementary materials to TITLE"
author: "Martin Papenberg"
output:
  pdf_document: default
  html_document:
    df_print: paged
bibliography: lit.bib
---

```{r include = FALSE}
# Set some knitr options
library(knitr)
library(anticlust)
library(here)
opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center", echo = FALSE)

tt <- read.csv(here("Application", "sample_data.csv"))
N <- nrow(tt)

numeric_features <- tt[, c("age", "BMI")]
categorical_features <- tt[, c("race", "ethnicity", "clinical_site", 
       "anatomical_site", "disease_stage", "cycle_phase", "sample_type", "lesion_type")]

```

## Standard algorithm for anticlustering

To assign samples to batches, we use anticlustering. Anticlustering is an optimization method that is characterized by (a) an objective function that quantifies the balance among batches, and (b) an algorithm that conducts the batch assignment in such a way that balance among batches is maximized via the objective function. Anticlustering owes its name to the fact that the objective functions it uses are the reversal of criteria used in cluster analysis. For example, @spath1986 already recognized that by maximizing instead of minimizing the k-means criterion (the "variance"), he was able to create groups that are similar to each other, and presented it as an improvement over the more intuitive random assignment. 

In our application, we optimize the (a) diversity objective using (b) the exchange algorithm by @papenberg2020. This constellation of objective and algorithm is also the default method implemented in the R package `anticlust` [@R-anticlust]. The diversity is defined as the total sum the within-cluster sums of dissimilarities [@brusco2019]. When assigning samples in such a way that the diversity is maximized, we simultaneously maximize similarity between batches. @papenberg2020 referred to the maximization of the diversity as "anticluster editing" because the minimization of the diversity is also well-known from the area of cluster analysis---under the term "cluster editing" [@shamir2004vf; @bocker2011]. 

The anticlustering exchange algorithm takes as input (1) a distance matrix, quantifying the pairwise dissimilarity among samples, and (2) the number of batches $K$. As measure of dissimilarity, we use the common Euclidean distance, which transfers the features of our samples to pairwise distances. It is defined as 

$$
d(x, y) =  \sqrt{\sum\limits_{i = 1}^{M}(x_i - y_i)^2}
$$

where $M$ is the number of numeric features describing two samples $x$ = ($x_1, \ldots, x_M$) and $y$ = ($y_1, \ldots, y_M$). When samples are described by two features, the Euclidean distance corresponds to the geometric, "straightline" distance between two points in a two-dimensional space; more similar data points are closer to each other. Figure&nbsp;1 illustrates the computation of the Euclidean distance and the diversity for the numeric features BMI and age that were used in our motivating application. For categorical variables, we use binary coding before including them in the computation of the Euclidean distance. Table 1 illustrates binary coding for the feature race in our data set, which had five unique values: `r paste0("(", letters[1:5], ") ", unique(tt$race))`. Such a categorical variable can be recoded into four binary variables to maintain the information included in the original variable. 

```{r, fig.width = 6, fig.height = 6, fig.cap = "Illustrates the conversion from numeric features to Euclidean distance, and (anti)clustering assignments based on minimum and maximum diversity using the Euclidean distance. Panel A illustrates the BMI and age of twelve women in our synthetic data in a scatter plot. Panel B represents the Euclidean distances between features as a straight line in the two-dimensional space. The Euclidean distance is proportional to the length of the connecting lines in panel B. Panel C illustrates a clustering assignment of the 12 data points to $K = 3$ equal-sized groups via \\textit{minimum} diversity. Panel D illustrates an anticlustering assignment of the 12 data points to $K = 3$ equal-sized groups via \\textit{maximum} diversity. The diversity is computed as the sum of within-(anti)cluster distances, which are highlighted in Panel C and Panel D through connecting lines."}

# Select 12 points ("arbitrarily")       
not_duplicated <- !duplicated(numeric_features)
six_randos <- c(28L, 144L, 108L, 102L, 7L, 138L, 54L, 147L, 1L, 87L, 110L, 31L)
points <- numeric_features[not_duplicated, ][six_randos, ]


## GENERATE FIGURE 1
standard <- c(5, 4, 4, 2) + 0.1
par(mfrow = c(2, 2))
size_letter <- 1.5

### A
par(mar = c(0, standard[2], standard[3], 0))
plot(points, las = 1, xaxt = "n", xlab = "")
legend("topleft", title = "A", bty = "n", legend = "", title.cex = size_letter)

### B
par(mar = c(0, 0, standard[3], standard[4]))
plot(points, las = 1, xaxt = "n", yaxt = "n")
anticlust:::draw_clique(
  points[,1], points[,2], col = "black",
  lwd = .7, lty = 2)
legend("topleft", title = "B", bty = "n", legend = "", title.cex = size_letter)

### C
par(mar = c(standard[1], standard[2], 0, 0))
clusters <- balanced_clustering(points, K = 3, method = "ilp")
colors <- c("#a9a9a9", "#df536b", "#61d04f")
cex <-  c(0.7, 1.2, 1.5)
pch <- c(19, 15, 17)

# Plot the data while visualizing the different clusters
plot(
  points,
  col = colors[clusters],
  pch = pch[clusters],
  cex = cex[clusters],
  las = 1, xlab = "Age"
)
legend("topleft", title = "C", bty = "n", legend = "", title.cex = size_letter)
anticlust:::draw_all_cliques(
  points[,1], points[,2], 
  assignment = clusters, 
  cols = colors[clusters], 
  lty = 2, lwd = .5
)


### D
par(mar = c(standard[1], 0, 0, standard[4]))

anticlusters <- anticlustering(points, K = 3, method = "ilp")
plot(
  points,
  col = colors[anticlusters],
  pch = pch[anticlusters],
  yaxt = "n", xlab = "Age"
)
legend("topleft", title = "D", bty = "n", legend = "", title.cex = size_letter)
anticlust:::draw_all_cliques(
  points[,1], points[,2], 
  assignment = anticlusters, 
  cols = colors[anticlusters], 
  lty = 2, 
  lwd = .7
)


```

```{r}
Race <- categorical_features[not_duplicated, c("race")][six_randos]
Race[grepl("Pacific Islander", Race)] <- "Pacific Islander"

mat <-  cbind(Race, categories_to_binary(Race))
rownames(mat) <- mat[,1]
knitr::kable(
  mat[,-1], 
  col.names = gsub("categories", "", colnames(mat)), 
  caption = "Illustrate the recoding of the variable Race using four binary variables."
)
```

The exchange algorithm that maximizes the diversity consists of two steps: an initialization step and an optimization step. As initialization, it randomly assigns samples to $K$ equal-sized batches. In principle, unequal-sized batches would also be possible with our algorithm, but equal-sized batches were required in the current application. After initialization, the algorithm selects the first sample and checks how the diversity would change if the sample were swapped with each sample that is currently assigned to a different batch. After simulating each exchange---that is $N - \frac{N}{K}$ exchanges---it realizes the one exchange that increases the diversity the most. It does not conduct an exchange if no improvement in diversity is possible. This procedure is repeated for each sample, leading to the evaluation of $N \cdot \frac{N}{K}$ exchanges in total; it terminates after the last sample was processed. The procedure might also restart at the first element and reiterate through all samples until no exchange leads to an improvement any more, i.e., until a local maximum is found. In `anticlust`, we also implemented this local maximum search, which corresponds to the algorithm LCW by @weitz1998. For better results, it is also possible to restart the search algorithm multiple times using different (random) initializations [@spath1986]. 

## Including must-link constraints with anticlustering

```{r}
must_link_frequencies <- table(table(tt$patientID))
```

To include must-link constraints with anticlustering, we adjusted both the initialization phase as well as the optimization phase of the exchange algorithm. A critical adjustment for both phases is that it treats a group of samples that must be assigned to the same batch as a single unit, because we recognize that not all samples can be considered independently of the other samples. In our application, samples belonging to the same patient were required to be assigned to the same batch; we refer to a group of samples that must be assigned to same batch as a *must-link cluster*. In our application synthetic data set, we simulated `r nrow(tt)` samples from `r length(unique(tt$patientID))` unique patients, which corresponds to the effective sample size for the anticlustering algorithm that includes must-link constraints. Most patients (*n* = `r must_link_frequencies[1]`) were included with 1 sample. The remaining `r nrow(tt) - must_link_frequencies[1]` were part of a must-link cluster with varying size between 2 (frequency = `r must_link_frequencies[2]`) and 8 (frequency = `r must_link_frequencies[7]`). 

### Initialization

During initialization, we assigns a weight to each patient, corresponding to the number of samples she provided. Using this conceptualization, the initialization step corresponds to a bin packing problem (citation needed): Each patient must has to be assigned to one of the $K$ batches without exceeding the maximum capacity of each batch (which is $\frac{N}{K}$ for each batch). We use a randomized fit heuristic to fill the batches: For each must-link cluster, we iterate through the $K$ batches in random order and assign it to the first batch where it fits.[^optimal] The process is expected to evenly distribute the must-link clusters among batches, and the random component is particularly useful if we use multiple restarts for the exchange algorithm. After assigning the must-link clusters to batches, the remaining samples can be assigned randomly to fill the remaining space.

[^optimal]: The randomized fit algorithm is a heuristic that may not yield a valid assignment of the must-link clusters even if one is theoretically available. If the heuristic indicates that the batches do cannot hold the must-link clusters, we verify this result using an optimal algorithm based on integer linear programming and use the optimal assignment if a solution is returned [@brusco2013]. Even though bin packing is a computationally difficult problem in theory (it is NP complete), we can use integer linear programming to process rather large data sets in reasonable time.

### Optimization

The optimization phase of the exchange algorithm is also based on a downscaled data set where each must-link cluster corresponds to a single unit. So, in our application, the original data set consists of `r nrow(tt)` samples, but the optimization algorithm works on a data set of unique patients. To obtain the reduced distance matrix that however preserves all information of the original distance matrix, we sum all pairwise distances between elements in different must-link clusters [@bocker2011]. This transformation preserves the computation of the diversity as compared to using the full data set. The exchange heuristic is then conducted on the merged elements. During the exchange process, we only exchange must-link clusters of the same size to ensure that the cardinality constraints are still respected in the end (i.e., equal-sized groups).


## Anticlustering vs. OSAT

- Some more details on the simulation study
- Given the parameters of the simulation, anticlustering strongly outperforms the OSAT method

## Optimal algorithm for anticlustering under cannot-link constraints

- Solving anticlustering optimally (i.e., finding a batch assignment with globally optimal value in diversity) under must-link constraints is possible using the ILP by @papenberg2020
- We have to adjust the input matrix: Samples that must be linked receive sufficiently large pairwise distance; in this case, a globally optimal solution must pair these samples in the same batch (if the constraints can be fulfilled at all)
- We investigate the feasibiliy of the optimal approach, which solves a computationally difficult (NP hard) problem (it seems to scale to about 40-50 samples; for larger data sets, a heuristic is required)
- We can use the optimal algorithm to validate the quality of our heuristic (how often does it find the globally optimal solution)

## References
