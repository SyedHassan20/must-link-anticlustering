---
title: "Supplementary materials to TITLE"
author: "Martin Papenberg"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
bibliography: lit.bib
---

```{r include = FALSE}
# Set some knitr options
library(knitr)
library(anticlust)
library(here)
library(tidyverse)
library(patchwork)
library(kableExtra)
opts_chunk$set(message = FALSE, warning = FALSE, fig.align = "center", echo = FALSE)

df <- read.csv(here("Application", "sample_data.csv"))
N <- nrow(df)

numeric_features <- df[, c("age", "BMI")]
categorical_features <- df[, c("race", "ethnicity", "clinical_site", 
       "anatomical_site", "disease_stage", "cycle_phase", "sample_type", "lesion_type")]

```

## Design

### Motivating example

```{r}
must_link_frequencies <- table(table(df$patientID))
N_unique <- length(unique(df$patientID))
```

- Explain data set, and why the application is needed
- Apply anticlustering to assign batches

The synthetic data set that we generated to resemble our actual application consisted of `r N` samples from `r N_unique` unique patients. In our application, samples belonging to the same patient were required to be assigned to the same batch. Most patients (*n* = `r must_link_frequencies[1]`) were included with 1 sample. The remaining `r N - must_link_frequencies[1]` patients provided more than one sample: 
`r must_link_frequencies[2]` patients provided 2 samples, 
`r must_link_frequencies[3]` patients provided 3 samples, 
`r must_link_frequencies[4]` patients provided 4 samples, 
`r must_link_frequencies[5]` patients provided 5 samples, 
`r must_link_frequencies[6]` patients provided 6 samples, 
and `r must_link_frequencies[7]` patient provided 8 samples.
We required the samples to be assigned to 16 equal-sized batches. As a perfect split was not possible using this constellation, we created 2 batches containing 24 samples and 14 batches containing 23 samples. We strived for balance among the batches with regard to two numeric variables (age, BMI) and several categorical variables (such as ethniciy, desease stage, and cycle phase). Table 1 illustrates the results of three batch assignments using three of the variables (age, BMI, ethnicity) for illustrative purposes. The first assignment is based on "standard" anticlustering, which ignores the must-link constraints and assumes that all samples can be assigned independently with the objective of maximum balance. This unrestricted anticlustering leads to the most pronounced balance among batches. The second assignment implements the must-link restrictions that samples belonging to the same patient have to be assigned to the same batch, while still optimizing balance via anticlsutering. The constrained assignment necessarily reduces the overall similarity among batches, but arguably the numeric variables in particular are still well balanced. The third assignment is based on a is a completely random allocation of samples to batches, which was included for comparative purporses. It by far leads to the worst balancing. 

In the next section, we will explain the methodology of anticlustering that led to the batch assignment illustrated in Table 1. We will reiterate the general anticlustering methodology first, before outlining how we included must-link constraints with anticlustering. The inclusion of must-link constraints with anticlustering is first done in this paper. 

```{r}

kmeans_input <- cbind(numeric_features, categories_to_binary(categorical_features)) 

nrep <- 100

df$Batch <- anticlustering(
  dist(kmeans_input)^2, 
  K = 16,
  method = "local-maximum",
  repetitions = nrep
)

df$Batch2 <- anticlustering(
  dist(kmeans_input)^2, 
  K = 16,
  method = "local-maximum",
  repetitions = nrep,
  must_link = df$patientID
)

df$Random_Batch <- sample(df$Batch)

tab1 <- df |> 
  group_by(Batch) |> 
  summarise(
    "Mean Age" = mean(age), 
    "Mean BMI" = mean(BMI), 
    "% Hispanic" =  paste0(mean(ethnicity == "Hispanic or Latino") |> round(2) * 100, "%")
  )
tab1$`Mean Age` <- prmisc::force_decimals(tab1$`Mean Age`)
tab1$`Mean BMI` <- prmisc::force_decimals(tab1$`Mean BMI`)

tab2 <- df |> 
  group_by(Batch2) |> 
  summarise(
    "Mean Age" = mean(age), 
    "Mean BMI" = mean(BMI), 
    "% Hispanic" = paste0(mean(ethnicity == "Hispanic or Latino") |> round(2) * 100, "%")
  )
tab2$`Mean Age` <- prmisc::force_decimals(tab2$`Mean Age`)
tab2$`Mean BMI` <- prmisc::force_decimals(tab2$`Mean BMI`)
tab3 <- df |> 
  group_by(Random_Batch) |> 
  summarise(
    "Mean Age" = mean(age), 
    "Mean BMI" = mean(BMI), 
    "% Hispanic" = paste0(mean(ethnicity == "Hispanic or Latino") |> round(2) * 100, "%")
  )
tab3$`Mean Age` <- prmisc::force_decimals(tab3$`Mean Age`)
tab3$`Mean BMI` <- prmisc::force_decimals(tab3$`Mean BMI`)

tab <- cbind(Batch = 1:16, cbind(tab1[, -1], tab2[, -1], tab3[, -1]))
tab |>
  kbl(booktabs = TRUE, align = "r", caption = "Example application") |>
  kable_styling(latex_options = c("striped", "scale_down")) |>
  add_header_above(
    c(" " = 1, "Anticlustering" = 3, "Constrained Anticlustering" = 3, "Random" = 3)
  )
```


### Standard algorithm for anticlustering

To assign samples to batches, we use anticlustering. Anticlustering is an optimization method that is characterized by (a) an objective function that quantifies the balance among batches, and (b) an algorithm that conducts the batch assignment in such a way that balance among batches is maximized via the objective function. Anticlustering owes its name to the fact that the objective functions it uses are the reversal of criteria used in cluster analysis. For example, @spath1986 already recognized that by maximizing instead of minimizing the k-means criterion (the "variance"), he was able to create groups that are similar to each other, and presented it as an improvement over the more intuitive random assignment.

In our application, we optimize the (a) diversity objective using (b) the exchange algorithm as described by @papenberg2020. This constellation of objective and algorithm is also the default method implemented in the R package `anticlust` [@R-anticlust]. The diversity is computed on the basis of a measure of pairwise dissimilarities among samples. In particular, it is defined as the overall sum of all dissimilarities among samples that are assigned to the same batch [@brusco2019]. As measure of dissimilarity, we use the common Euclidean distance, which transfers the features of our samples to pairwise distances. It is defined as 

$$
d(x, y) =  \sqrt{\sum\limits_{i = 1}^{M}(x_i - y_i)^2}
$$

where $M$ is the number of numeric features describing two samples $x$ = ($x_1, \ldots, x_M$) and $y$ = ($y_1, \ldots, y_M$). When samples are described by two features, the Euclidean distance corresponds to the geometric, "straightline" distance between two points in a two-dimensional space; more similar data points are closer to each other. Figure&nbsp;1 illustrates the computation of the Euclidean distance and the computation diversity for the numeric features BMI and age that were used in our motivating application. For categorical variables, we use binary coding before including them in the computation of the Euclidean distance (see Table 2 for an example). 

An anticlustering algorithm assigns samples to batches in such a way that the diversity is maximized. Importantly, maximizing the diversity within batches simultaneously maximize similarity among batches, leading to a balanced distribution of the input variables among batches. @papenberg2020 referred to the maximization of the diversity as "anticluster editing" because the minimization of the diversity is also well-known from the area of cluster analysis---under the term "cluster editing" [@shamir2004vf; @bocker2011]. 

The anticlustering exchange algorithm takes as input (1) a distance matrix, quantifying the pairwise dissimilarity among samples, and (2) the number of batches $K$. In most anticlustering applications, samples are split into equal-sized groups $\frac{N}{K}$, which was also the requirement in our motivating application. 

```{r, fig.width = 6, fig.height = 6, fig.cap = "Illustrates the conversion from numeric features to Euclidean distance, and (anti)clustering assignments based on minimum and maximum diversity using the Euclidean distance. Panel A illustrates the BMI and age of twelve women in our synthetic data in a scatter plot. Panel B represents the Euclidean distances between features as a straight line in the two-dimensional space. The Euclidean distance is proportional to the length of the connecting lines in panel B. Panel C illustrates a clustering assignment of the 12 data points to $K = 3$ equal-sized groups via \\textit{minimum} diversity. Panel D illustrates an anticlustering assignment of the 12 data points to $K = 3$ equal-sized groups via \\textit{maximum} diversity. The diversity is computed as the sum of within-(anti)cluster distances, which are highlighted in Panel C and Panel D through connecting lines."}

# Select 12 points ("arbitrarily")       
not_duplicated <- !duplicated(numeric_features)
six_randos <- c(28L, 144L, 108L, 102L, 7L, 138L, 54L, 147L, 1L, 87L, 110L, 31L)
points <- numeric_features[not_duplicated, ][six_randos, ]


## GENERATE FIGURE 1
standard <- c(5, 4, 4, 2) + 0.1
par(mfrow = c(2, 2))
size_letter <- 1.5

### A
par(mar = c(0, standard[2], standard[3], 0))
plot(points, las = 1, xaxt = "n", xlab = "")
legend("topleft", title = "A", bty = "n", legend = "", title.cex = size_letter)

### B
par(mar = c(0, 0, standard[3], standard[4]))
plot(points, las = 1, xaxt = "n", yaxt = "n")
anticlust:::draw_clique(
  points[,1], points[,2], col = "black",
  lwd = .7, lty = 2)
legend("topleft", title = "B", bty = "n", legend = "", title.cex = size_letter)

### C
par(mar = c(standard[1], standard[2], 0, 0))
clusters <- balanced_clustering(points, K = 3, method = "ilp")
colors <- c("#a9a9a9", "#df536b", "#61d04f")
cex <-  c(0.7, 1.2, 1.5)
pch <- c(19, 15, 17)

# Plot the data while visualizing the different clusters
plot(
  points,
  col = colors[clusters],
  pch = pch[clusters],
  cex = cex[clusters],
  las = 1, xlab = "Age"
)
legend("topleft", title = "C", bty = "n", legend = "", title.cex = size_letter)
anticlust:::draw_all_cliques(
  points[,1], points[,2], 
  assignment = clusters, 
  cols = colors[clusters], 
  lty = 2, lwd = .5
)


### D
par(mar = c(standard[1], 0, 0, standard[4]))

anticlusters <- anticlustering(points, K = 3, method = "ilp")
plot(
  points,
  col = colors[anticlusters],
  pch = pch[anticlusters],
  yaxt = "n", xlab = "Age"
)
legend("topleft", title = "D", bty = "n", legend = "", title.cex = size_letter)
anticlust:::draw_all_cliques(
  points[,1], points[,2], 
  assignment = anticlusters, 
  cols = colors[anticlusters], 
  lty = 2, 
  lwd = .7
)


```

```{r}
Race <- categorical_features[not_duplicated, c("race")][six_randos]
Race[grepl("Pacific Islander", Race)] <- "Pacific Islander"

mat <-  cbind(Race, categories_to_binary(Race))
rownames(mat) <- mat[,1]
knitr::kable(
  mat[!duplicated(mat), -1], 
  col.names = gsub("categories", "", colnames(mat)), 
  caption = "Illustrates the recoding of the categorical variable Race using four binary variables."
)
```

The exchange algorithm that maximizes the diversity consists of two steps: an initialization step and an optimization step. As initialization, it randomly assigns samples to $K$ equal-sized batches. In principle, unequal-sized batches would also be possible, but equal-sized batches were required in the current application. After initialization, the algorithm selects the first sample and checks how the diversity would change if the sample were swapped with each sample that is currently assigned to a different batch. After simulating each exchange---that is $N - \frac{N}{K}$ exchanges---it realizes the one exchange that increases the diversity the most. It does not conduct an exchange if no improvement in diversity is possible. This procedure is repeated for each sample, leading to the evaluation of $N \cdot \frac{N}{K}$ exchanges in total; it terminates after the last sample was processed. The procedure might also restart at the first element and reiterate through all samples until no exchange leads to an improvement any more, i.e., until a local maximum is found. In `anticlust`, we also implemented this local maximum search, which corresponds to the algorithm LCW by @weitz1998. For better results, it is also possible to restart the search algorithm multiple times using different (random) initializations [@spath1986]. 

## Including must-link constraints with anticlustering

In our application, samples belonging to the same patient were required to be assigned to the same batch. We refer to a set of samples that must be assigned to same batch as a must-link *clique*. In our application, not all samples were part of a clique. For the data set that resembled our actual application, we simulated `r N` samples from `r N_unique` unique patients. Most patients (*n* = `r must_link_frequencies[1]`) were included with 1 sample; these samples were not part of a clique. The remaining `r N - must_link_frequencies[1]` patients were part of a clique, with clique sizes varying between 2 and 8: `r must_link_frequencies[2]` cliques of size 2, `r must_link_frequencies[3]` cliques of size 3, `r must_link_frequencies[4]` cliques of size 4, `r must_link_frequencies[5]` cliques of size 5, `r must_link_frequencies[6]` cliques of size 6, and `r must_link_frequencies[7]` clique of size 8. 

To include must-link constraints with anticlustering, we use a downscaled data set where each unique patient---but not each single sample---constitutes a unit in the anticlustering process. Hence, in our application, the effective sample size was `r N_unique` instead of `r N`. Some adjustments of the exchange method are required to ensure (a) we still obtain a valid partitioning regarding the size constraints (equal-sized batches) and (b) the diversity is computed correctly during optimization. We therefore had to adjust both the initialization phase as well as the optimization phase of the exchange algorithm. 

During initialization, we first assign all samples to a batch that are part of a clique (`r N - must_link_frequencies[1]` samples). Each clique must be assigned completely to one of the $K$ batches and samples within the clique must not be split apart. At the same time, the maximum capacity of each batch must not be exceeded. Using this conceptualization, the initialization step corresponds to a bin packing problem, which is one the classical NP complete problem in computer science [@garey1979]. We assign a weight to each clique, corresponding to the number of samples it contains. This leads there are $K$ batches each with capacity $\frac{N}{K}$, which have to be filled with cliques. The sum of the weights of the cliques in each batch must not exceed its capacity. Many optimal and heuristic algorithms have been developed to address the bin packing problem. as default method, we use a randomized fit heuristic to fill the batches. For each must-link cluster, we iterate through the $K$ batches in random order and assign it to the first batch where it fits. The process is expected to evenly distribute the must-link clusters among batches, and the random component is particularly useful if we use multiple restarts for the exchange algorithm. After assigning the must-link clusters to batches, the remaining samples can be assigned randomly to fill the remaining space. Note that our randomized fit algorithm is a heuristic that may not find an assignment of must-link groups to batches even if one is theoretically available. If the heuristic indicates that the batches cannot hold the must-link groups, we therefore use an optimal algorithm based on integer linear programming as fallback option, which allows us to verify if the constraints really cannot be fulfilled [@martello1990].
 
The optimization phase of the exchange algorithm also uses the downscaled data set where each patient rather than each sample corresponds corresponds to a unit of analysis. To obtain the reduced distance matrix that preserves all information of the original distance matrix, we sum up all pairwise distances between samples in different cliques [@bocker2011]. This transformation preserves the computation of the diversity as compared to using the full data set, because the diversity itself is defined as a sum of distances within-cliques. The exchange heuristic is then conducted on the level of patients. During the exchange process, we only exchange cliques of the same size to ensure that the cardinality constraints are respected throughout (i.e., equal-sized groups).

## Simulation Study


```{r}
tt <- read.csv(here("Simulation_vs_OSAT", "results.csv"), sep = ";")
tt$ID <- 1:nrow(tt)
p_values <- tt[, grepl("p_", colnames(tt))]

# global results OSAT vs. anticlust. How often is p value of anticlust better (i.e. larger)?
anticlust_better <- apply(p_values, 1, function(x) x[1:5] < x[6:10])
osat_better <- apply(p_values, 1, function(x) x[1:5] > x[6:10])
same <- apply(p_values, 1, function(x) x[1:5] == x[6:10])

anticlust_better_percent <- round(mean(anticlust_better, na.rm = TRUE) * 100)
osat_better_percent <- round(mean(osat_better, na.rm = TRUE) * 100)
same_percent <- round(mean(same, na.rm = TRUE) * 100)

avg_constrained_obj <- round(mean(tt$diversity_constrained / tt$diversity_unconstrained) * 100, 1)
constrained_not_worse <- round(mean(apply(p_values, 1, function(x) x[6:10] <= x[11:15]), na.rm = TRUE)*100)

```

We conducted a simulation study to illustrate the usefulness of anticlustering for batch assignment. The simulation followed two goals. First, it compared anticlustering with the OSAT package, which is currently the gold standard for automated batch assignment (citation). Second, it investigated whether the quality of batch assignment is reduced with must-link constraints as compared to an unconstrained assignment. We generated `r nrow(tt)` data sets, which were all processed via (a) the default OSAT algorithm (optimal shuffle with 5000 repetitions), (b) unconstrained anticlustering and (c) anticlustering subject to must-link constraints. In each data set, we randomly determined the number of categorical variables (2-5), the number of classes per variable (2-5), the total sample size (between 50 and 500) and the number of batches (2, 5, or 10 equal-sized batches). Must-link constraints were randomly generated in such a way that the distribution of constraints resembled our motivating application: 58% of all elements had no must-link partner; 29% had 1 must-link partner; 10% had 2 must-link partners, and 3% had 3 or more must-link partners. The code and data to reproduce the simulation and analysis is openly available via XXX (**TODO**). 

For each simulation run, we computed $\chi^2$-tests to assess the imbalance among batches for each of the 2-5 variables, for each of the three competing methods. We stored the *p* value associated with each test. A higher *p* value indicates that there is less imbalance among batches, i.e., that the batches are more similar. The simulation revealed that in `r anticlust_better_percent`% of all variable comparisons, balance among batches was better when using anticlustering as compared to OSAT. Balance was equal in `r same_percent`% of all comparisons, and only in `r osat_better_percent`% OSAT outperformed `anticlust`. These results clearly underline the power of anticlustering for batch assignment. Moreover, the anticlustering assignment that was subjected to must-link constraints on average achieved `r avg_constrained_obj`% of the objective value of the unconstrained assignment. Hence, must-link constraints are not only desirable from a user's point of view, but they also do not decrease batch balance considerably; in `r constrained_not_worse`% of all cases, balance was not at all reduced by the constraints. Figure 2 illustrates the average balance among batches in dependence of the factors that varied between data sets. Must-link constraints primarily reduced balance in smaller data sets, but did not strongly affect the balance in larger data sets. Remarkably, the constrained anticlustering assignment led to better balance than the OSAT assignment that did not employ any constraints. Figure 2 also shows increasing the number of variables as well as increasing the number of categories per variables posed severe challenges for OSAT, but hardly affected anticlustering.

```{r, fig.cap= "Average \\textit{p} values in dependence of the factors that varied in the simulation study. Higher \\textit{p} values indicate better balance. Anticlustering maintained a comparable level of balance in all conditions. OSAT's performance decreased with increasing number of variables and number of categories per variable."}

## Some more sophisticated analyes:
dfl <- tt |> 
  select(ID, N, M, K, P, starts_with("p")) |> 
  pivot_longer(
    cols = starts_with("p_")
  ) |> 
  na.omit()

dfl$Method <- "Anticlustering"
dfl$Method[grepl("osat", dfl$name)] <- "OSAT"
dfl$Method[grepl("p_anticlust_c", dfl$name)] <- "Must-Link Anticlustering"

dfl$N_category <- santoku::chop(dfl$N, breaks = c(100, 200, 300, 400))
dfl$K <- ordered(dfl$K)
dfl$M <- ordered(dfl$M)
dfl$P <- ordered(dfl$P)

p0 <- dfl |> 
  group_by(Method, N_category) |> 
  summarise(`p value` = mean(value)) |> 
  ggplot(aes(y = `p value`, x = N_category, color = Method)) +
  geom_point(aes(color = Method, shape = Method)) + 
  theme_bw(base_size = 8) +
  xlab("Sample size (categorized)") +
  ylab("Average p value")

p1 <- dfl |> 
  group_by(Method, M) |> 
  summarise(`p value` = mean(value)) |> 
  ggplot(aes(y = `p value`, x = M, color = Method)) +
  geom_point(aes(color = Method, shape = Method)) + 
  theme_bw(base_size = 8) +
  xlab("Number of variables") +
  ylab("")

p2 <- dfl |> 
  group_by(Method, K) |> 
  summarise(`p value` = mean(value)) |> 
  ggplot(aes(y = `p value`, x = K, color = Method)) +
  geom_point(aes(color = Method, shape = Method)) +
  theme_bw(base_size = 8) +
  xlab("Number of batches") +
  ylab("Average p value")

p3 <- dfl |> 
  group_by(Method, P) |> 
  summarise(`p value` = mean(value)) |> 
  ggplot(aes(y = `p value`, x = P, color = Method)) +
  geom_point(aes(color = Method, shape = Method)) +
  theme_bw(base_size = 8) +
  xlab("Number of categories per variable") +
  ylab("")

p0 + p1 + p2 + p3+ plot_layout(guides = "collect", widths = c(2, 1)) 
```

## Optimal algorithm for anticlustering under cannot-link constraints

- Solving anticlustering optimally (i.e., finding a batch assignment with globally optimal value in diversity) under must-link constraints is possible using the ILP by @papenberg2020
- We have to adjust the input matrix: Samples that must be linked receive sufficiently large pairwise distance; in this case, a globally optimal solution must pair these samples in the same batch (if the constraints can be fulfilled at all)
- We investigate the feasibiliy of the optimal approach, which solves a computationally difficult (NP hard) problem (it seems to scale to about 40-50 samples; for larger data sets, a heuristic is required)
- We can use the optimal algorithm to validate the quality of our heuristic (how often does it find the globally optimal solution)

\clearpage

## References
